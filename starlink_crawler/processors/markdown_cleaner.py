#!/usr/bin/env python3
"""
Markdown Cleaner for Starlink Crawler.

This module provides functionality to clean and process markdown files
generated by the Starlink crawler. It can be used both as part of the
crawling process or as a standalone post-processing tool.

Features:
- Extract metadata (title, URL, article ID, category)
- Clean content (remove navigation, fix URLs)
- Generate standardized YAML frontmatter
- Rename files using article IDs
"""

import os
import re
import yaml
from datetime import datetime
from typing import Dict, Optional, Tuple, List, Any


class MarkdownCleaner:
    """Class for cleaning and processing Starlink markdown content."""

    def __init__(self, 
                 extract_metadata: bool = True,
                 fix_urls: bool = True,
                 remove_navigation: bool = True,
                 remove_footer: bool = True,
                 add_frontmatter: bool = True,
                 language: str = 'en'):
        """
        Initialize the MarkdownCleaner with configuration options.
        
        Args:
            extract_metadata: Whether to extract metadata from content
            fix_urls: Whether to fix malformed URLs
            remove_navigation: Whether to remove navigation elements
            remove_footer: Whether to remove footer content
            add_frontmatter: Whether to add YAML frontmatter
            language: Default language for the content
        """
        self.extract_metadata = extract_metadata
        self.fix_urls = fix_urls
        self.remove_navigation = remove_navigation
        self.remove_footer = remove_footer
        self.add_frontmatter = add_frontmatter
        self.language = language

    def extract_title(self, content: str) -> Optional[str]:
        """
        Extract the main article title from the markdown content.
        
        Args:
            content: The markdown content
            
        Returns:
            The title if found, None otherwise
        """
        # Split content into lines
        lines = content.split('\n')
        
        # First try to extract from frontmatter if present
        in_frontmatter = False
        for line in lines:
            if line.strip() == '---':
                if not in_frontmatter:
                    in_frontmatter = True
                else:
                    break
            elif in_frontmatter and line.startswith('title:'):
                title = line[6:].strip().strip('"\'')
                if title:
                    return title
        
        # Then try to find the first ### heading (this is usually the most accurate title)
        for line in lines:
            if line.startswith('### '):
                # Extract the full title from the H3 heading
                full_title = line[4:].strip()
                # Some titles end with a question mark, which is part of the title
                return full_title
        
        # Return None if no title is found
        return None

    def extract_article_id(self, url: Optional[str]) -> Optional[str]:
        """
        Extract article ID from the article URL.
        
        Args:
            url: The article URL
            
        Returns:
            The article ID if found, None otherwise
        """
        # Check if URL is provided
        if not url:
            return None
        
        # Use regex to extract 36-character UUID
        match = re.search(r'/([a-f0-9-]{36})', url)
        
        # Return the extracted UUID if found, otherwise None
        return match.group(1).lower() if match else None

    def extract_category(self, content: str) -> Optional[str]:
        """
        Extract category from the markdown content.
        
        Args:
            content: The markdown content
            
        Returns:
            The category if found, None otherwise
        """
        # Split content into lines
        lines = content.split('\n')
        categories = []
        
        # First try to extract from frontmatter if present
        in_frontmatter = False
        for line in lines:
            if line.strip() == '---':
                if not in_frontmatter:
                    in_frontmatter = True
                else:
                    break
            elif in_frontmatter and line.startswith('category:'):
                category = line[9:].strip().strip('"\'')
                if category:
                    return category
        
        # Then try to find categories in the content
        for line in lines[:20]:
            if line.strip().startswith('* ') and not line.strip().startswith('* ['):
                category = line.strip('* ').strip()
                if category and category != 'Articles':
                    categories.append(category)
        
        # Return the first category found, or None if none
        return categories[0] if categories else None

    def extract_url(self, content: str, title: Optional[str]) -> Optional[str]:
        """
        Extract URL for the current article.
        
        Args:
            content: The markdown content
            title: The article title
            
        Returns:
            The URL if found, None otherwise
        """
        # Check if title is provided
        if not title:
            return None
            
        # First try to extract from frontmatter if present
        lines = content.split('\n')
        in_frontmatter = False
        for line in lines:
            if line.strip() == '---':
                if not in_frontmatter:
                    in_frontmatter = True
                else:
                    break
            elif in_frontmatter and line.startswith('url:'):
                url = line[4:].strip().strip('"\'')
                if url:
                    return url
        
        # Clean the title for comparison
        clean_title = title.strip()
        
        # Try exact match first
        pattern = f'\\[{re.escape(clean_title)}\\]\\(([^\\)]+)\\)'
        match = re.search(pattern, content)
        
        # If no match, try with optional spaces
        if not match:
            pattern = f'\\[{re.escape(clean_title)}\\s*\\]\\(([^\\)]+)\\)'
            match = re.search(pattern, content)
        
        # Return the URL if found, otherwise None
        if match:
            url = match.group(1)
            # Clean up the URL
            url = url.rstrip('>')
            return url
        return None

    def clean_content(self, content: str) -> str:
        """
        Clean up Starlink-specific markdown content.
        
        Args:
            content: Original markdown content
            
        Returns:
            Cleaned markdown content with:
            - Removed all content before main title (###)
            - Fixed malformed URLs
            - Removed unnecessary whitespace
            - Removed footer content
            - Removed 'On this page' sections
        """
        # Split content into lines for processing
        lines = content.split('\n')
        cleaned_lines = []
        found_main_title = False
        found_footer = False
        in_on_this_page = False
        
        for line in lines:
            # Once we find the main title (###), start including content
            if line.startswith('### '):
                found_main_title = True
                
            # Check for 'On this page' section start
            if line.strip() == '##### On this page':
                in_on_this_page = True
                continue
                
            # Check if we're leaving the 'On this page' section
            if in_on_this_page and line.strip() and not line.strip().startswith('* '):
                in_on_this_page = False
                
            # Check for footer start
            if "Can't find what you're looking for?" in line:
                found_footer = True
                continue
                
            # Skip lines if conditions are met
            if (self.remove_navigation and not found_main_title) or \
               (self.remove_footer and found_footer) or \
               in_on_this_page:
                continue
                
            # Fix various malformed URLs if enabled
            if self.fix_urls:
                if '<#' in line:
                    # Fix anchor links: <#africa> -> #africa
                    line = line.replace('<#', '#').replace('>', '')
                elif '</support/article/' in line:
                    # Fix article URLs: </support/article/UUID> -> /support/article/UUID
                    line = line.replace('</support/article/', '/support/article/')
                    line = line.replace('>', '')
                elif 'https://www.starlink.com/support/article/<' in line:
                    # Fix embedded URLs in markdown links
                    line = line.replace('https://www.starlink.com/support/article/<', '')
                    line = line.replace('>', '')
                
            cleaned_lines.append(line)
        
        # Remove any leading/trailing empty lines
        while cleaned_lines and not cleaned_lines[0].strip():
            cleaned_lines.pop(0)
        while cleaned_lines and not cleaned_lines[-1].strip():
            cleaned_lines.pop()
            
        return '\n'.join(cleaned_lines)

    def get_metadata(self, content: str) -> Dict[str, Any]:
        """
        Extract all metadata from the content.
        
        Args:
            content: The markdown content
            
        Returns:
            Dictionary of metadata
        """
        if not self.extract_metadata:
            return {}
        
        # Extract the full title from the content
        title = self.extract_title(content)
        
        # For articles with H3 headings, make sure we get the complete title
        # by looking directly at the content
        lines = content.split('\n')
        for line in lines:
            if line.startswith('### '):
                # This is the most accurate title - use the full H3 heading
                title = line[4:].strip()
                break
                
        url = self.extract_url(content, title) if title else None
        article_id = self.extract_article_id(url)
        
        # Construct the standardized Starlink support URL
        page_url = f'https://www.starlink.com/support/article/{article_id}' if article_id else url
        
        metadata = {
            'url': page_url,
            'title': title,
            'article_id': article_id,
            'category': self.extract_category(content),
            'last_modified': datetime.now().strftime('%Y-%m-%d'),
            'language': self.language
        }

        # Remove None values
        metadata = {k: v for k, v in metadata.items() if v is not None}
        
        return metadata

    def create_frontmatter(self, metadata: Dict[str, Any]) -> str:
        """
        Create YAML frontmatter from metadata.
        
        Args:
            metadata: Dictionary of metadata
            
        Returns:
            YAML frontmatter as string
        """
        if not self.add_frontmatter or not metadata:
            return ""
        
        # Instead of using yaml.dump which can cause line breaks in long strings,
        # we'll manually create the YAML frontmatter for better control
        frontmatter_lines = ["---"]
        
        # Process each metadata item
        for key, value in metadata.items():
            if key == 'title' and value:
                # For titles, ensure no line breaks and proper quoting
                title = value.replace('\n', ' ').strip()
                # Use explicit double quotes for title to ensure it's treated as a single line
                frontmatter_lines.append(f"title: \"{title}\"")
            elif isinstance(value, str):
                # For other string values, add quotes if they contain special characters
                if any(c in value for c in ":\n#'\""):
                    frontmatter_lines.append(f"{key}: '{value}'")
                else:
                    frontmatter_lines.append(f"{key}: {value}")
            else:
                # For non-string values, use standard representation
                frontmatter_lines.append(f"{key}: {value}")
        
        frontmatter_lines.append("---\n")
        return "\n".join(frontmatter_lines)

    def process_content(self, content: str) -> Tuple[str, Dict[str, Any]]:
        """
        Process markdown content and return cleaned content with metadata.
        
        Args:
            content: Original markdown content
            
        Returns:
            Tuple of (processed_content, metadata)
        """
        # Extract metadata first (before cleaning)
        metadata = self.get_metadata(content)
        
        # Clean the content
        cleaned_content = self.clean_content(content)
        
        # Create frontmatter if needed
        frontmatter = self.create_frontmatter(metadata)
        
        # Combine frontmatter and cleaned content
        processed_content = f"{frontmatter}{cleaned_content}"
        
        return processed_content, metadata

    def process_file(self, input_path: str, output_dir: Optional[str] = None) -> Tuple[str, str]:
        """
        Process a single markdown file and create a new version with metadata.
        
        Args:
            input_path: Path to the input markdown file
            output_dir: Directory to save the processed file (if None, don't save)
            
        Returns:
            Tuple of (output_path, article_id)
        """
        # Read the markdown file
        with open(input_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Process the content
        processed_content, metadata = self.process_content(content)
        
        # If no output directory is specified, just return the processed content
        if not output_dir:
            return processed_content, metadata.get('article_id', '')
        
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        # Determine output filename
        article_id = metadata.get('article_id', '')
        if article_id:
            output_filename = f"{article_id}.md"
        else:
            # Fallback to original filename if no article_id found
            output_filename = os.path.basename(input_path)
        
        # Save to new file
        output_path = os.path.join(output_dir, output_filename)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(processed_content)
        
        return output_path, article_id

    def process_directory(self, input_dir: str, output_dir: str) -> List[Tuple[str, str]]:
        """
        Process all markdown files in a directory.
        
        Args:
            input_dir: Directory containing markdown files
            output_dir: Directory to save processed files
            
        Returns:
            List of (output_path, article_id) tuples
        """
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        # Find all markdown files
        md_files = [f for f in os.listdir(input_dir) if f.endswith('.md')]
        results = []
        
        # Process each file
        for filename in md_files:
            input_path = os.path.join(input_dir, filename)
            output_path, article_id = self.process_file(input_path, output_dir)
            results.append((output_path, article_id))
        
        return results


# Command-line interface functions
def process_single_file(input_path: str, output_dir: str = 'starlink_markdown_output_files_clean') -> str:
    """
    Process a single markdown file using the default cleaner.
    
    Args:
        input_path: Path to the input markdown file
        output_dir: Directory to save the processed file
        
    Returns:
        Output path
    """
    cleaner = MarkdownCleaner()
    output_path, article_id = cleaner.process_file(input_path, output_dir)
    return output_path


def process_directory(input_dir: str, output_dir: str = 'starlink_markdown_output_files_clean') -> List[str]:
    """
    Process all markdown files in a directory using the default cleaner.
    
    Args:
        input_dir: Directory containing markdown files
        output_dir: Directory to save processed files
        
    Returns:
        List of output paths
    """
    cleaner = MarkdownCleaner()
    results = cleaner.process_directory(input_dir, output_dir)
    return [path for path, _ in results]


# Main function for standalone use
def main():
    """Process markdown files based on command line arguments."""
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description='Clean and process Starlink markdown files')
    parser.add_argument('input', nargs='?', help='Input file or directory')
    parser.add_argument('--output-dir', '-o', default='starlink_markdown_output_files_clean',
                        help='Output directory for processed files')
    parser.add_argument('--no-metadata', action='store_true', help='Skip metadata extraction')
    parser.add_argument('--no-fix-urls', action='store_true', help='Skip URL fixing')
    parser.add_argument('--keep-navigation', action='store_true', help='Keep navigation elements')
    parser.add_argument('--keep-footer', action='store_true', help='Keep footer content')
    parser.add_argument('--no-frontmatter', action='store_true', help='Skip adding YAML frontmatter')
    parser.add_argument('--language', default='en', help='Content language (default: en)')
    
    args = parser.parse_args()
    
    # Create cleaner with specified options
    cleaner = MarkdownCleaner(
        extract_metadata=not args.no_metadata,
        fix_urls=not args.no_fix_urls,
        remove_navigation=not args.keep_navigation,
        remove_footer=not args.keep_footer,
        add_frontmatter=not args.no_frontmatter,
        language=args.language
    )
    
    # Process input
    if not args.input:
        # Default to processing all files in the default directory
        input_dir = 'starlink_markdown_output_files'
        if not os.path.isdir(input_dir):
            print(f"Error: Default directory {input_dir} not found")
            return 1
            
        # Count total markdown files
        md_files = [f for f in os.listdir(input_dir) if f.endswith('.md')]
        total_files = len(md_files)
        
        print(f"\nFound {total_files} markdown files to process in {input_dir}")
        print("This may take a while for large numbers of files.")
        
        # Ask for confirmation
        response = input(f"\nDo you want to proceed with processing all {total_files} files? (y/n): ")
        if response.lower() != 'y':
            print("Operation cancelled by user.")
            return 0
        
        # Process each markdown file
        for i, filename in enumerate(md_files, 1):
            input_path = os.path.join(input_dir, filename)
            print(f"Processing {filename}... ({i}/{total_files})")
            output_path, article_id = cleaner.process_file(input_path, args.output_dir)
            
        print(f"\nProcessing complete! {total_files} files saved in {args.output_dir}/")
        
    elif os.path.isfile(args.input):
        # Process single file
        input_path = args.input
        output_path, article_id = cleaner.process_file(input_path, args.output_dir)
        print(f"Processed {os.path.basename(input_path)} -> {output_path}")
        
    elif os.path.isdir(args.input):
        # Process directory
        input_dir = args.input
        md_files = [f for f in os.listdir(input_dir) if f.endswith('.md')]
        total_files = len(md_files)
        
        print(f"\nFound {total_files} markdown files to process in {input_dir}")
        
        # Process each markdown file
        for i, filename in enumerate(md_files, 1):
            input_path = os.path.join(input_dir, filename)
            print(f"Processing {filename}... ({i}/{total_files})")
            output_path, article_id = cleaner.process_file(input_path, args.output_dir)
            
        print(f"\nProcessing complete! {total_files} files saved in {args.output_dir}/")
        
    else:
        print(f"Error: Input {args.input} not found")
        return 1
        
    return 0


if __name__ == '__main__':
    sys.exit(main())
